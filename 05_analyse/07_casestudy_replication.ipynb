{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook shows the approach that was chosen for the practical examples in the study \"Comprehensive Validation of Word Embeddings for Social Science Research\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the following packages are needed\n",
    "import os\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### needed functions\n",
    "\n",
    "def load_model(file_path):\n",
    "    \"\"\"Loads a word embedding model from the given file path.\"\"\"\n",
    "    if file_path.endswith('.bin'):\n",
    "        model = gensim.models.fasttext.load_facebook_vectors(file_path)\n",
    "    elif file_path.endswith('.vec'):\n",
    "        model = gensim.models.KeyedVectors.load_word2vec_format(file_path, binary=False)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_path}\")\n",
    "    return model\n",
    "\n",
    "def get_nearest_neighbors(model, keyword, top_n=100):\n",
    "    \"\"\"Fetches the top N nearest neighbors for a given keyword from the model.\"\"\"\n",
    "    return model.most_similar(keyword, topn=top_n)\n",
    "\n",
    "# Function to calculate percentage overlap\n",
    "def calculate_percentage_overlap(df):\n",
    "    overlap_matrix = pd.DataFrame(index=df.columns, columns=df.columns, dtype=float)\n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            set1 = set(df[col1])\n",
    "            set2 = set(df[col2])\n",
    "            overlap = len(set1.intersection(set2))\n",
    "            percentage_overlap = (overlap / len(set1)) * 100\n",
    "            overlap_matrix.loc[col1, col2] = percentage_overlap\n",
    "    return overlap_matrix\n",
    "\n",
    "# Function to count word presence\n",
    "def count_word_presence(df):\n",
    "    word_presence = {}\n",
    "    word_models = {}\n",
    "    for col in df.columns:\n",
    "        for word in df[col]:\n",
    "            if word in word_presence:\n",
    "                word_presence[word] += 1\n",
    "                word_models[word].append(col)\n",
    "            else:\n",
    "                word_presence[word] = 1\n",
    "                word_models[word] = [col]\n",
    "    return word_presence, word_models\n",
    "\n",
    "# Function to filter word presence\n",
    "def filter_word_presence(word_presence, min_models=3):\n",
    "    return {word: count for word, count in word_presence.items() if count >= min_models}\n",
    "\n",
    "# Function to plot heatmap with modified labels and custom color scheme\n",
    "def plot_heatmap(overlap_matrix, filename):\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    # Create a colormap with different hues of the color #0063A6\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_blue\", [\"#ffffff\", \"#0063A6\"])\n",
    "    sns.heatmap(overlap_matrix, annot=True, fmt=\".0f\", cmap=cmap, xticklabels=short_labels(overlap_matrix.columns), yticklabels=short_labels(overlap_matrix.index))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.savefig(filename, format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "# Function to plot word presence with modified labels and filter\n",
    "def plot_word_presence(word_presence, min_models, filename):\n",
    "    filtered_word_presence = filter_word_presence(word_presence, min_models)\n",
    "    presence_df = pd.DataFrame.from_dict(filtered_word_presence, orient='index', columns=['count'])\n",
    "    presence_df = presence_df.sort_values(by='count', ascending=False)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(x=presence_df.index, y=presence_df['count'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.savefig(filename, format='pdf')\n",
    "    plt.show()\n",
    "\n",
    "# Helper function to get first two words of column names\n",
    "def short_labels(labels):\n",
    "    return ['_'.join(label.split('_')[:2]) for label in labels]\n",
    "\n",
    "# Function to calculate percentage overlap and average overlap\n",
    "def calculate_percentage_overlap_and_average(df):\n",
    "    overlap_matrix = pd.DataFrame(index=df.columns, columns=df.columns, dtype=float)\n",
    "    total_overlap = 0\n",
    "    num_pairs = 0\n",
    "    \n",
    "    for col1 in df.columns:\n",
    "        for col2 in df.columns:\n",
    "            set1 = set(df[col1])\n",
    "            set2 = set(df[col2])\n",
    "            overlap = len(set1.intersection(set2))\n",
    "            percentage_overlap = (overlap / len(set1)) * 100\n",
    "            overlap_matrix.loc[col1, col2] = percentage_overlap\n",
    "            \n",
    "            if col1 != col2:\n",
    "                total_overlap += percentage_overlap\n",
    "                num_pairs += 1\n",
    "    \n",
    "    average_overlap = total_overlap / num_pairs if num_pairs > 0 else 0\n",
    "    print(f\"Average overlap: {average_overlap:.2f}%\")\n",
    "    \n",
    "    return overlap_matrix\n",
    "\n",
    "def find_words_in_all_columns(words_table):\n",
    "    word_locations = defaultdict(set)\n",
    "    # Iterate through each column and record word occurrences\n",
    "    for col in words_table.columns:\n",
    "        for word in words_table[col].dropna():  # dropna() to skip NaN values\n",
    "            word_locations[word].add(col)\n",
    "    # Group words by the number of columns they appear in\n",
    "    words_by_column_count = defaultdict(list)\n",
    "    for word, columns in word_locations.items():\n",
    "        words_by_column_count[len(columns)].append((word, columns))\n",
    "    return words_by_column_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define keywords and their variants\n",
    "keywords = {\n",
    "    'frau': ('frau', 'Frau'),\n",
    "    'femizid': ('femizid', 'Femizid')\n",
    "}\n",
    "\n",
    "model_dir = 'path_to_models'\n",
    "file_type = '.vec'\n",
    "\n",
    "# Loop through each keyword\n",
    "for keyword, (keyword_lower, keyword_cased) in keywords.items():\n",
    "    # Create an empty DataFrame to store all neighbors\n",
    "    all_neighbors = pd.DataFrame(columns=['Model', 'Neighbor', 'Similarity'])\n",
    "\n",
    "    # List to store the frequency of each neighbor\n",
    "    neighbor_counter = Counter()\n",
    "\n",
    "    # Load each model and get nearest neighbors\n",
    "    for model_file in os.listdir(model_dir):\n",
    "        if model_file.endswith(file_type):\n",
    "            model_path = os.path.join(model_dir, model_file)\n",
    "            model_name = os.path.splitext(model_file)[0]\n",
    "            print(f\"Processing model: {model_name} for keyword: {keyword}\")\n",
    "\n",
    "            if 'lower' in model_name:\n",
    "                selected_keyword = keyword_lower\n",
    "            else:\n",
    "                selected_keyword = keyword_cased\n",
    "\n",
    "            model = load_model(model_path)\n",
    "            neighbors = get_nearest_neighbors(model, selected_keyword)\n",
    "\n",
    "            for neighbor, similarity in neighbors:\n",
    "                neighbor_lower = neighbor.lower()\n",
    "                all_neighbors = all_neighbors.append({'Model': model_name, 'Neighbor': neighbor_lower, 'Similarity': similarity}, ignore_index=True)\n",
    "                neighbor_counter[neighbor_lower] += 1\n",
    "\n",
    "    # Save the neighbors to a CSV file\n",
    "    all_neighbors.to_csv(f'table_allneighbours_{keyword}.csv', index=False)\n",
    "\n",
    "    # Save the frequency count of neighbors\n",
    "    with open(f'neighbor_counts_{keyword}.txt', 'w') as f:\n",
    "        for neighbor, count in neighbor_counter.most_common():\n",
    "            f.write(f\"{neighbor}: {count}\\n\")\n",
    "\n",
    "    ### Prepare Table of Nearest Neighbors for Analysis\n",
    "\n",
    "    df = pd.read_csv(f'table_allneighbours_{keyword}.csv')\n",
    "\n",
    "    # Count of all words in the Neighbor column\n",
    "    word_count = df['Neighbor'].nunique()\n",
    "\n",
    "    # Creating a new table with each model as a column\n",
    "    grouped_df = df.groupby('Model')['Neighbor'].apply(list).reset_index()\n",
    "\n",
    "    # Convert the lists to columns\n",
    "    max_neighbors = grouped_df['Neighbor'].apply(len).max()\n",
    "    new_df = pd.DataFrame({model: neighbors + [None] * (max_neighbors - len(neighbors)) \n",
    "                        for model, neighbors in zip(grouped_df['Model'], grouped_df['Neighbor'])})\n",
    "\n",
    "    # Transpose the DataFrame so each model is a column\n",
    "    new_df = new_df.T.reset_index(drop=True).T\n",
    "\n",
    "    # Assign columns names\n",
    "    new_df.columns = grouped_df['Model']\n",
    "\n",
    "    print(f\"Count of all unique words: {word_count}\")\n",
    "    new_df.to_csv(f'table_allneighbours_{keyword}_pivot.csv')\n",
    "\n",
    "    \"\"\"At this point the 'neighbor_counts_{keyword}.txt' file was used to manually create a dictionnary, which will serve to combined words, that are the same, except for their grammatical strucutre e.g. (woman and women)\"\"\"\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('table_allneighbours_{keyword}_pivot.csv', sep=',', index_col='Unnamed: 0')\n",
    "\n",
    "    # Load the manual lemmatization dictionary\n",
    "    wf = pd.read_csv('path_to_dict', sep=';') \n",
    "    lemma_dict = dict(zip(wf['original_word'], wf['lemma_word']))\n",
    "\n",
    "    def translate_word(word):\n",
    "        return lemma_dict.get(word, word)  # Return the original word if not found in dictionary\n",
    "\n",
    "    # Apply the translation function to the table\n",
    "    lemmatized_df = df.applymap(translate_word)\n",
    "\n",
    "    # Calculate percentage overlap for original data\n",
    "    original_percentage_overlap = calculate_percentage_overlap(df)\n",
    "    plot_heatmap(original_percentage_overlap, f'original_overlap_heatmap_{keyword}.pdf')\n",
    "\n",
    "    # Lemmatize neighbors using the manual dictionary and calculate percentage overlap\n",
    "    lemmatized_percentage_overlap = calculate_percentage_overlap(lemmatized_df)\n",
    "    plot_heatmap(lemmatized_percentage_overlap, f'lemmatized_overlap_heatmap_{keyword}.pdf')\n",
    "\n",
    "    # Calculate word presence for original data\n",
    "    original_word_presence, original_word_models = count_word_presence(df)\n",
    "    plot_word_presence(original_word_presence, min_models=16, filename=f'original_word_presence_{keyword}.pdf')\n",
    "\n",
    "    ###### some code parts to get descriptives\n",
    "    words_in_all_columns = find_words_in_all_columns(lemmatized_df)\n",
    "\n",
    "    total_words = 0\n",
    "    for column_count in sorted(words_in_all_columns.keys(), reverse=True):\n",
    "        words_list = words_in_all_columns[column_count]\n",
    "        total_words += len(words_list)\n",
    "        print(f\"The following words are in {column_count} columns (Total: {len(words_list)}):\")\n",
    "        for word, columns in words_list:\n",
    "            print(f\"  Word: {word} appears in columns: {', '.join(columns)}\")\n",
    "        print()\n",
    "\n",
    "    print(f\"Total number of words for {keyword}: {total_words}\")\n",
    "\n",
    "    # words in half the columns or fewer\n",
    "    half_columns = len(lemmatized_df.columns) // 2\n",
    "    words_in_half_or_less = sum(len(words_in_all_columns[count]) for count in range(1, half_columns + 1))\n",
    "\n",
    "    print(f\"Number of words that are in half the columns or fewer for {keyword}: {words_in_half_or_less}\")\n",
    "\n",
    "    # words in more than 75% of the columns\n",
    "    seventy_five_percent_columns = len(lemmatized_df.columns) * 0.75\n",
    "    words_in_seventy_five_percent_or_more = sum(len(words_in_all_columns[count]) for count in range(int(seventy_five_percent_columns) + 1, len(lemmatized_df.columns) + 1))\n",
    "\n",
    "    print(f\"Number of words that are in more than 75% of the columns for {keyword}: {words_in_seventy_five_percent_or_more}\")\n",
    "\n",
    "    # average overlap\n",
    "    calculate_percentage_overlap_and_average(lemmatized_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "janaenv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
